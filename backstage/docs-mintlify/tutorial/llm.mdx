---
title: Connecting to LLM
description: Add AI capabilities to your Vertz app
---

# Step 6: Connecting to LLM

Now let's connect our chatbot to an LLM for intelligent responses.

## Install LLM Package

```bash
npm install @vertz/llm
```

## Configure the LLM

Create `src/server/llm.ts`:

```typescript
import { createLlm } from '@vertz/llm';

export const llm = createLlm({
  provider: 'openai',
  model: 'gpt-4o',
  apiKey: process.env.OPENAI_API_KEY,
});
```

## Update the Chat Router

Update `src/server/routers/chat.ts`:

```typescript
import { router, procedure, streamingProcedure } from '@vertz/server';
import { z } from '@vertz/schema';
import { Conversation, Message, query } from '../db/entities';
import { llm } from '../llm';

export const chatRouter = router({
  // ... existing code ...
  
  // Stream AI response
  sendMessage: procedure
    .input(z.object({
      conversationId: z.string().uuid(),
      content: z.string(),
    }))
    .mutation(async ({ input, ctx }) => {
      // Save user message
      const userMessage = await Message.create({
        data: {
          conversationId: input.conversationId,
          role: 'user',
          content: input.content,
          createdAt: new Date(),
        },
      });
      
      // Get conversation history
      const conversation = await query(Conversation)
        .where({ id: input.conversationId })
        .include({ messages: { orderBy: { createdAt: 'asc' } } })
        .first();
      
      // Build messages for LLM
      const llmMessages = conversation.messages.map((m) => ({
        role: m.role,
        content: m.content,
      }));
      
      // Generate response
      const response = await llm.chat(llmMessages);
      
      // Save assistant message
      const assistantMessage = await Message.create({
        data: {
          conversationId: input.conversationId,
          role: 'assistant',
          content: response.content,
          createdAt: new Date(),
        },
      });
      
      return {
        userMessage,
        assistantMessage,
      };
    }),
  
  // Streaming version
  sendMessageStream: streamingProcedure
    .input(z.object({
      conversationId: z.string().uuid(),
      content: z.string(),
    }))
    .query(async ({ input, emit }) => {
      // Save user message
      const userMessage = await Message.create({
        data: {
          conversationId: input.conversationId,
          role: 'user',
          content: input.content,
          createdAt: new Date(),
        },
      });
      
      // Stream the response
      let fullResponse = '';
      
      for await (const chunk of llm.stream(input.content)) {
        fullResponse += chunk.content;
        emit(chunk);
      }
      
      // Save complete message
      await Message.create({
        data: {
          conversationId: input.conversationId,
          role: 'assistant',
          content: fullResponse,
          createdAt: new Date(),
        },
      });
    }),
});
```

## System Prompts

Add a system prompt for context:

```typescript
const systemPrompt = `You are a helpful customer support assistant.
You help users with their questions about our product.
Be concise, friendly, and helpful.`;

const response = await llm.chat([
  { role: 'system', content: systemPrompt },
  ...llmMessages,
]);
```

## Tools (Advanced)

Enable the LLM to use tools:

```typescript
import { tool } from '@vertz/llm';

const getWeather = tool({
  description: 'Get the current weather',
  parameters: z.object({
    location: z.string(),
  }),
  execute: async ({ location }) => {
    // Call weather API
    return { temperature: 72, condition: 'sunny' };
  },
});

const response = await llm.chat(llmMessages, {
  tools: [getWeather],
});
```

## Next Steps

Let's deploy the application in [Step 7: Deployment](/tutorial/deployment).
